{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjDF21LDq5wk",
        "outputId": "2c7d5af7-ac6b-4c22-cc51-6a392d8e6f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkJavsuBK9PX",
        "outputId": "8e015954-7d33-43c1-b879-f61c99ea294b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (5,006 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 120903 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n"
          ]
        }
      ],
      "source": [
        "#!pip3 install pytesseract\n",
        "#!pip3 install tesseract\n",
        "#!pip3 install pillow\n",
        "\n",
        "# Then install Tesseract using apt-get\n",
        "!apt-get install tesseract-ocr\n",
        "!pip install pytesseract Pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1s4eUuzhr5e",
        "outputId": "fac2be28-444f-4414-e35b-43f3318599a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gTTS\n",
            "  Downloading gTTS-2.4.0-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gTTS) (2.31.0)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gTTS) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2023.11.17)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.4.0\n",
            "Requirement already satisfied: gTTS in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gTTS) (2.31.0)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gTTS) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install gTTS\n",
        "!pip install --upgrade gTTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cxeFBC9PT8_"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "from PIL import Image\n",
        "#import pytesseract\n",
        "from gtts import gTTS\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "import IPython.display as ipd\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "kQPoXjoX4W3u",
        "outputId": "8830ab42-9c3d-4f86-f6cf-03e9e8681c66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Give the video path: /content/drive/MyDrive/Thesis_practice/Code 2/Result/input/street.mp4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-bd10992cc333>\u001b[0m in \u001b[0;36m<cell line: 248>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    280\u001b[0m        \u001b[0mclasses_to_look_for\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_look_for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mstart_video_object_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-bd10992cc333>\u001b[0m in \u001b[0;36mstart_video_object_detection\u001b[0;34m(video)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 \u001b[0mdetected_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_yolo_object_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;31m# Application of object recognition methods on a video frame from YOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-bd10992cc333>\u001b[0m in \u001b[0;36mapply_yolo_object_detection\u001b[0;34m(image_to_process)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;31m# Compare with ground truth boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mgt_box\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mground_truth_boxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                     \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_iou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myolo_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0miou\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# You can adjust the IoU threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ground_truth_boxes' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "#from art import tprint\n",
        "#import pytesseract\n",
        "\n",
        "def play_audio(text):\n",
        "\n",
        "    tts = gTTS(text)\n",
        "    tts.save(\"/content/drive/MyDrive/Thesis_practice/Code 2/outputaudio.mp3\")  # Save the audio to a file (replace \"/content/output.mp3\" with your desired file path)\n",
        "\n",
        "def apply_yolo_object_detection(image_to_process):\n",
        "    \"\"\"\n",
        "    Recognition and determination of the coordinates of objects on the image\n",
        "    :param image_to_process: original image\n",
        "    :return: image with marked objects and captions to them\n",
        "    \"\"\"\n",
        "\n",
        "    height, width, _ = image_to_process.shape\n",
        "    blob = cv2.dnn.blobFromImage(image_to_process, 1 / 255, (608, 608),\n",
        "                                 (0, 0, 0), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    outs = net.forward(out_layers)\n",
        "    class_indexes, class_scores, boxes = ([] for i in range(3))\n",
        "    objects_count = 0\n",
        "    detected_objects = []\n",
        "    true_positives = []\n",
        "\n",
        "    # Starting a search for objects in an image\n",
        "    for out in outs:\n",
        "        for obj in out:\n",
        "            scores = obj[5:]\n",
        "            class_index = np.argmax(scores)\n",
        "            class_score = scores[class_index]\n",
        "            if class_score > 0:\n",
        "                center_x = int(obj[0] * width)\n",
        "                center_y = int(obj[1] * height)\n",
        "                obj_width = int(obj[2] * width)\n",
        "                obj_height = int(obj[3] * height)\n",
        "                box = [center_x - obj_width // 2, center_y - obj_height // 2,\n",
        "                       obj_width, obj_height]\n",
        "                #boxes.append(box)\n",
        "                class_name = classes[class_index]\n",
        "                detected_objects.append((class_name,box))\n",
        "                yolo_box = [center_x - obj_width // 2, center_y - obj_height // 2, obj_width, obj_height]\n",
        "\n",
        "\n",
        "\n",
        "                #class_indexes.append(class_index)\n",
        "                #class_scores.append(float(class_score))\n",
        "\n",
        "    return detected_objects\n",
        "\n",
        "    # Selection\n",
        "    # chosen_boxes = cv2.dnn.NMSBoxes(boxes, class_scores, 0.0, 0.4)\n",
        "    # for box_index in chosen_boxes:\n",
        "    #     box_index = box_index\n",
        "    #     box = boxes[box_index]\n",
        "    #     class_index = class_indexes[box_index]\n",
        "\n",
        "    #     # For debugging, we draw objects included in the desired classes\n",
        "    #     if classes[class_index] in classes_to_look_for:\n",
        "    #         objects_count += 1\n",
        "    #         image_to_process = draw_object_bounding_box(image_to_process,\n",
        "    #                                                     class_index, box)\n",
        "\n",
        "    # final_image = draw_object_count(image_to_process, objects_count)\n",
        "    # return final_image\n",
        "\n",
        "\n",
        "def draw_object_bounding_box(image_to_process, index, box):\n",
        "    \"\"\"\n",
        "    Drawing object borders with captions\n",
        "    :param image_to_process: original image\n",
        "    :param index: index of object class defined with YOLO\n",
        "    :param box: coordinates of the area around the object\n",
        "    :return: image with marked objects\n",
        "    \"\"\"\n",
        "\n",
        "    x, y, w, h = box\n",
        "    start = (x, y)\n",
        "    end = (x + w, y + h)\n",
        "    color = (0, 255, 0)\n",
        "    width = 2\n",
        "    final_image = cv2.rectangle(image_to_process, start, end, color, width)\n",
        "\n",
        "    start = (x, y - 10)\n",
        "    font_size = 1\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    width = 2\n",
        "    text = classes[index]\n",
        "    final_image = cv2.putText(final_image, text, start, font,\n",
        "                              font_size, color, width, cv2.LINE_AA)\n",
        "\n",
        "    return final_image\n",
        "\n",
        "\n",
        "def draw_object_count(image_to_process, objects_count):\n",
        "    \"\"\"\n",
        "    Signature of the number of found objects in the image\n",
        "    :param image_to_process: original image\n",
        "    :param objects_count: the number of objects of the desired class\n",
        "    :return: image with labeled number of found objects\n",
        "    \"\"\"\n",
        "\n",
        "    start = (10, 120)\n",
        "    font_size = 1.5\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    width = 3\n",
        "    text = \"Objects found: \" + str(objects_count)\n",
        "\n",
        "    # Text output with a stroke\n",
        "    # (so that it can be seen in different lighting conditions of the picture)\n",
        "    white_color = (255, 255, 255)\n",
        "    black_outline_color = (0, 0, 0)\n",
        "    final_image = cv2.putText(image_to_process, text, start, font, font_size,\n",
        "                              black_outline_color, width * 3, cv2.LINE_AA)\n",
        "    final_image = cv2.putText(final_image, text, start, font, font_size,\n",
        "                              white_color, width, cv2.LINE_AA)\n",
        "\n",
        "    return final_image\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def start_video_object_detection(video: str):\n",
        "    \"\"\"\n",
        "    Захват и анализ видео в режиме реального времени\n",
        "    \"\"\"\n",
        "\n",
        "    # Define video output parameters\n",
        "    while True:\n",
        "        try:\n",
        "            # Capturing a picture from a video\n",
        "            video_camera_capture = cv2.VideoCapture(video)\n",
        "            class_counts = {class_name: 0 for class_name in classes}\n",
        "\n",
        "\n",
        "            while video_camera_capture.isOpened():\n",
        "                ret, frame = video_camera_capture.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                detected_objects = apply_yolo_object_detection(frame)\n",
        "\n",
        "                # Application of object recognition methods on a video frame from YOLO\n",
        "                #detected_objects = apply_yolo_object_detection(frame)\n",
        "                #dire = r'/content/drive/MyDrive/Thesis_practice/output_image.jpg'  # You can set the desired output file name and extension\n",
        "                #cv2.imwrite(dire, frame)    # Assuming you're using OpenCV to work with images\n",
        "                #image = Image.open(dire)\n",
        "\n",
        "                #bbox_coords = (x, y, width, height)\n",
        "\n",
        "                #roi = image.crop(bbox_coords)\n",
        "                #try:\n",
        "                   #text = pytesseract.image_to_string(image, lang='eng')\n",
        "                   #print(text)\n",
        "                #except Exception as e:\n",
        "                   #print(\"An error occurred:\", str(e))\n",
        "                #filename=r'/content/drive/MyDrive/Thesis_practice/Code 2/output.txt'\n",
        "                #file1 = open(filename+\".txt\",\"w\")\n",
        "                #file1.write(text)\n",
        "                #file1.close()\n",
        "                #print(\"Done...\")\n",
        "\n",
        "                #im = Image.open(dire)\n",
        "                #text = pytesseract.image_to_string(im, lang=language)\n",
        "                #filename=input(\"Type output file name:\")\n",
        "                #file1 = open(filename+\"-\"+language+\".txt\",\"w\")\n",
        "                #file1.write(text)\n",
        "                #file1.close()\n",
        "                #print(\"Done...\")\n",
        "\n",
        "                # Displaying the processed image on the screen with a reduced window size\n",
        "                #recognizer = cv2.face.createLBPHFaceRecognizer()\n",
        "\n",
        "                #frame = cv2.resize(frame, (1920 // 2, 1080 // 2))\n",
        "                #cv2.cv2_imshow(\"Video Capture\", frame)\n",
        "                total_objects = 0\n",
        "                for class_name, box in detected_objects:\n",
        "                   x, y, w, h = box\n",
        "                   start = (x, y)\n",
        "                   end = (x + w, y + h)\n",
        "                   color = (0, 255, 0)\n",
        "                   width = 2\n",
        "                   frame = cv2.rectangle(frame, start, end, color, width)\n",
        "\n",
        "                   start = (x, y - 10)\n",
        "                   font_size = 1\n",
        "                   font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "                   width = 2\n",
        "                   text = class_name\n",
        "                   frame = cv2.putText(frame, text, start, font, font_size, color, width, cv2.LINE_AA)\n",
        "\n",
        "                   class_counts[class_name] += 1\n",
        "                   total_objects += 1\n",
        "\n",
        "\n",
        "               # Print the detected objects\n",
        "                text_to_speek = \"\"\n",
        "                text_speek=\"\"\n",
        "                arekta_text=\"\"\n",
        "                print(\"Object Count for Each Class:\")\n",
        "                text_speek += \"Object Count for Each Class\"\n",
        "\n",
        "\n",
        "                for class_name, count in class_counts.items():\n",
        "                    print(f\"{class_name}: {count}\")\n",
        "                    arekta_text = str(count) + class_name\n",
        "\n",
        "                print(\"\\nTotal Object Count:\", total_objects)\n",
        "                text_speek += \"Total Object Count\"\n",
        "                text_speek += str(total_objects)\n",
        "\n",
        "                for class_name, _ in detected_objects:\n",
        "                    print(f\"Detected: {class_name}\")\n",
        "                    text_to_speek= \"Detected\" + class_name\n",
        "                    text_speek += text_to_speek\n",
        "\n",
        "                play_audio(text_speek)\n",
        "\n",
        "\n",
        "                cv2_imshow(frame)\n",
        "\n",
        "                #cv2.cv2_imshow(cv2.resize(image_np, (800, 600)))\n",
        "\n",
        "                cv2.waitKey(1)\n",
        "\n",
        "            video_camera_capture.release()\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Logo\n",
        "    #tprint(\"Object detection\")\n",
        "    #tprint(\"by\")\n",
        "    #tprint(\"paveldat\")\n",
        "\n",
        "    # Loading YOLO scales from files and setting up the network\n",
        "    net = cv2.dnn.readNetFromDarknet(\"/content/drive/MyDrive/Thesis_practice/Code 2/Resources/yolov4-tiny.cfg\",\n",
        "                                   \"/content/drive/MyDrive/Thesis_practice/Code 2/Resources/yolov4-tiny.weights\")\n",
        "    layer_names = net.getLayerNames()\n",
        "    out_layers_indexes = net.getUnconnectedOutLayers()\n",
        "    out_layers = [layer_names[index - 1] for index in out_layers_indexes]\n",
        "\n",
        "    # Loading from a file of object classes that YOLO can detect\n",
        "    with open(\"/content/drive/MyDrive/Thesis_practice/Code 2/Resources/coco.names.txt\") as file:\n",
        "        classes = file.read().split(\"\\n\")\n",
        "\n",
        "    # Determining classes that will be prioritized for search in an image\n",
        "    # The names are in the file coco.names.txt\n",
        "\n",
        "    video = input(\"Give the video path: \")\n",
        "    #look_for = input(\"What are we looking for: \").split(',')\n",
        "    #/content/drive/MyDrive/Thesis_practice/Code 2/Result/input/street.mp4\n",
        "\n",
        "    # Delete spaces\n",
        "    look_for = [\"item1\", \"item2\", \"item3\"]\n",
        "\n",
        "    list_look_for = [\"all\"]\n",
        "    for look in look_for:\n",
        "       list_look_for.append(look.strip())\n",
        "\n",
        "       classes_to_look_for = list_look_for\n",
        "\n",
        "    start_video_object_detection(video)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}